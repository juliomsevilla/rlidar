[["index.html", "Tratamiento de datos LiDAR forestales mediante R Introducción", " Tratamiento de datos LiDAR forestales mediante R Julio M. Sevilla 2021-10-27 Introducción R es un lenguaje que originalmente se pensó por y para estadísticos. Hoy, se ha convertido en uno de los diez principales lenguajes de programación no siendo ninguno de estos cinco lenguajes puramente estadísticos, lo cual nos lleva a pensar que quizás ya no estemos ante lo que se pensó, y que ese niño ha crecido y se está haciendo mayor. El presente documento es un manual básico sobre el lenguaje de programación R, su fundamento, utilidad y consejos prácticos para su uso y su implementación para tratamiento de datos LiDAR forestales y la estimación de valores de la masa arbórea mediante métodos de masa y de árbol individual. Es importante recalcar lo básico del documento y que este no pretende ser una obra de referencia en el género: son unas notas, simples, accesibles y sin mayor aspiración de que sirvan para para que un neofito se introduzca en el tema y se despierte en él las ganas de aprender este lenguaje y sobre LiDAR forestal. Por ello, si deseas un conocimiento más profundo, te recomendamos que adquieras la bibliografía que se citará a lo largo de estas notas. La versión en línea de este libro está autorizada bajo la licencia the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License "],["introducción-al-lenguaje-r.html", "1 Introducción al lenguaje R 1.1 Introducción al lenguaje R 1.2 Instalación de R, RStudio y librerias 1.3 Actualización de R 1.4 Gestión de directorios, archivos y versiones 1.5 Manejo de objetos en R 1.6 Manejo de datos en R 1.7 Profundizando en el data.frame 1.8 Estructuras de control 1.9 Los gráficos en R 1.10 Las funciones en R 1.11 Publicación de resultados", " 1 Introducción al lenguaje R 1.1 Introducción al lenguaje R 1.1.1 ¿Que es R? R o The R Project for Statistical Computing (R Core Team 2020) es un lenguaje y entorno de programación de alto nivel, orientado a objetos y fundamentalmente centrado al análisis estadístico y gráfico, distribuido bajo la licencia GNU GPL y está disponible para los sistemas operativos Windows, Macintosh, Unix y GNU/Linux. El origen de R se remonta a 1993 cuando Robert Gentleman y Ross Ihaka del Departamento de Estadística de la Universidad de Auckland adaptan el lenguaje de programación S, propio de los famosos Bell Laboratories de AT&amp;T, el cual es un sistema para el análisis de datos desarrollado por John Chambers y Rick Becker fundamentalmente y usado desde finales de 1970 (Ross y Robert comienzan a llamar R al lenguaje que implementaron, por las iniciales de sus nombres). Es más, muchos de los libros y manuales sobre S son útiles para R. 1.1.2 ¿Por qué usar R? R tiene varias ventajas respecto a otros lenguajes de programación de fines estadísticos o softwares dedicados. Entre otras podemos citar: Sus posibilidades gráficas son excelentes y muy versátiles. Esto llega hasta el punto de que muchos que R tiene dos aspectos: el de software, vamos a llamarlo, analítico y el gráfico, pudiendo incluso independizarse su aprendizaje. Cuenta con una elevada flexibilidad. Los trabajos más sencillos o complejos algoritmos pueden ejecutarse escribiéndose una palabra. No es una caja negra y en todo momento sabemos que estamos haciendo. Este es un aspecto que no se valora como se debería cuando hacemos nuestros análisis estadísticos: a costa de la facilidad de hacer todo con un par de clicks perdemos fácilmente el control de lo que estamos haciendo (o incluso llegamos a desconocerlo), algo que no debería ser permitido, ya que quien hace un análisis busca una conclusión y no saber cómo se obtiene debería invalidar por si solo dicha conclusión. En R, un análisis estadístico se realiza por pasos que arrojan resultados que se van almacenando en objetos. De esta forma, estos pueden recuperarse y usarse en cualquier momento. Al contrario que otros software, permite salidas mínimas de resultados, fácilmente legibles y analizables, y no una salida copiosa de datos y gráficos. Salvo que utilicemos una GUI, su aspecto es minimalista y espartano a conciencia (hasta su web sigue esta filosofía), siguiendo la filosofía de no perder el foco de atención de lo que estamos haciendo con el marco en el que lo hacemos. Es libre, con filosofía y objetivos del proyecto GNU, con lo que podemos acceder al código escrito por otros usuarios y modificarlo libremente y evidentemente, podemos programar nuestras propios procedimientos y aplicaciones. Es un proyecto vivo con dos millones de usuarios. Seguramente a nuestro problema ya se habrá enfrentado alguien y casi con total seguridad no deberemos avanzar más de tres páginas de Google para encontrar la solución. Existen multitud de librerías (paquetes) programadas por los usuarios de todo el mundo para llevar a cabo procedimientos específicos (¡más de 10.000!). Y si con todo ello, no tenemos suficiente, es totalmente gratuito, tanto el como todo su ecosistema. 1.1.3 Recursos sobre R Para aprender sobre R y estar al corriente de las últimas novedades existen una serie de recursos que deberemos añadir a nuestros Favoritos. Entre otros: Página oficial de R: https://www.r-project.org/ Fundación R: https://www.r-project.org/foundation/ Manuales iniciales: https://cran.r-project.org/manuals.html Revista R: https://journal.r-project.org/ R-Bloggers: http://www.r-bloggers.com/ Comunidad R-Hispano: http://r-es.org/Comunidad Recursos MOOC como Coursera: www.coursera.org Bibliografía en papel: R for Everyone: Advanced Analytics and Graphics (http://www.jaredlander.com/r-for-everyone/), R Cookbook (http://shop.oreilly.com/product/9780596809164.do) y el clásico Modern Applied Statistics with S (http://www.springer.com/us/book/9780387954578) En este punto nos puede asaltar la duda ¿Markdowncubre todo el espectro que cubre HTML? Sí y no, porque si no lo cubre, tan solo tienes que escribir la parte de HTML` separado una línea del siguiente párrafo en Markdown. Trabajar con Markdown, tiene infinidad de ventajas, pero es especialmente adecuado para aquellas personas que publican contenido en la web de manera regular. Entre otras ventajas cabe citar que cuando te acostumbras a la sintaxis la generación de texto es extremadamente rápido, es muy sencillo de leer, no tendremos el problema de errores por no cerrar adecuadamente las etiquetas HTML y su compilación con software como Pandoc permite la exportación simple a infinidad de formatos basados en texto. 1.2 Instalación de R, RStudio y librerias 1.2.1 Instalación de R R funciona bajo Windows, Linux, Mac o Solaris. Para instalar R accederemos a su página (en este caso, un mirror de España): http://cran.es.r-project.org/ y descargaremos la versión adecuada a nuestro sistema operativo. En el caso de Windows, la instalación es muy sencilla y solo deberemos seguir los pasos que se nos indica. Si todo ha ido correctamente, cuando pulsemos sobre el icono de R, se abrirá su espartana interfaz en la que, además de la barra de herramientas, destaca la consola donde escribiremos y ejecutaremos nuestro código. Algo bastante interesante de R es su portabilidad. R, una vez instalado en un PC puede funcionar de forma autónoma al mismo. Por ello, una simple copia del directorio donde lo hayamos instalado y su posterior pegado en una memoria flash, nos permitirá ejecutarlo en cualquier lugar, llevándonos así nuestra configuración personal, paquetes, entornos y demás. 1.3 Actualización de R Cada cierto tiempo es recomendable realizar la actualización de R. Generalmente, un par de veces al año, el sistema es actualizado, misión que lleva a cabo un reducido grupo de especialistas (conocido como el R Core team) y los paquetes (que en próximos capítulos explicaremos) se adaptan a la nueva actualización, dejando, algunas veces de ser útiles en las versiones obsoletas. La actualización de R puede ser automatizada o realizarse de forma manual si bien este último recurso no es la mejor opción debido a que se actualizará creando una nueva instalación manteniendo la anterior. Este echo se debe a que en la actualidad es la única forma de que no perdamos nuestras configuraciones y, sobre todo, la posible incompatibilidad con alguno de los paquetes que tengamos instalados.Para realizar una actualización automática del sistema, podemos usar la siguiente sentencia usando el paquete (Galili 2021) (despues veremos el lugar de ejecutarla): if(!require(installr)) { install.packages(&quot;installr&quot;); require(installr)} updateR() 1.3.1 El Sistema R Una vez instalamos R, nos encontramos con una interfaz gráfica de usuario que nos puede resultar anticuada sobre todo si trabajamos con los sistemas operativos más recientes. Este aspecto no debe confundirnos de cuál es la visión del sistema R: un lenguaje de programación con una clara vocación estadístico-matemática. R 1.4 Gestión de directorios, archivos y versiones 1.4.1 Gestión de directorios Existen dos comandos básicos para trabajar con directorios: setwd() y getwd(). El primero, fija el directorio de trabajo que deseamos y el segundo lo llama. Con el directorio fijado, podremos movernos por él, usando el comando \"./data\" que nos haría descender a una carpeta llamada data que cuelga de nuestro directorio raíz fijado con setwd(). Por otro lado, si usamos el comando \"../data\" ascenderemos a una carpeta que esta en un nivel superior a nuestro directorio raíz. A continuación, podemos usar los comandos list.files() y list.dirs() para poder listar los archivos y directorios respectivamente que tengamos en el directorios elegido. Es interesante indicar que si añadimos la opción recursive=TRUE los dos comandos listaran todos los ficheros y archivos que haya tanto en nuestro directorio raiz como en todos los inferiores. Finalmente, usando el comando full.names = TRUE en vez de solo mostrarnos el nombre completo del fichero, nos mostrará toda su ruta. Ahora combinemos todo lo anterior para listar los ficheros de un directorio: list.files(&quot;c:/&quot;) list.dirs(&quot;c:/&quot;) Finalmente, con el comando pattern = podremos seleccionar determinados ficheros o directorios que coincidan con una determinada condición. Por ejemplo, pattern = \"(.*).jp2$\" nos mostrará todos los ficheros que finalicen la extensión jp2 y pattern = \"29SMD(.*)SCL_20(.*).jp2$\" nos listará todos los ficheros que además de finalizar con jp2incluyan la cadena 29SMD y SCL_20. Una vez tenemos claro los conceptos anteriores, podremos realizar trabajos recursivos con nuestros ficheros y directorios (tranquilo, comprenderás este código a lo largo de estos apuntes), por ejemplo: List &lt;- list.files(getwd(), full.names = TRUE, pattern = &quot;29SMD(.*)SCL_20(.*).jp2$&quot;, recursive=TRUE) for (file in List) { out_file &lt;- extension(file, &#39;tif&#39;) gdal_translate(src_dataset = file, dst_dataset = out_file, ot = &quot;UInt16&quot;, of = &quot;GTiff&quot;) } 1.4.2 Manejo de archivos Cuando cargamos un fichero externo, sea del formato que sea, R, por defecto lo interpretará como un data.frame (ver ??), si este viene en un formato usual de tabla. R puede cargar multitud de ficheros distintos y si no lo hace con las opciones incluidas en ´{base}´ lo hará a traves de librerias. Veamos a continuación los más frecuentes. 1.4.2.1 Ficheros csv Los ficheros delimitados por comas se cargan con el comando read.csv indicando al menos la ruta del mismo, si viene o no con cabeceras, el separador decimal y el separador de colummas: datos&lt;-read.csv(&quot;./datos.csv&quot;, header=TRUE, dec= &quot;.&quot;, sep=&quot;,&quot;) En el caso de no indicar, header, dec y/o sep, el comando read.csv importará el fichero con el primer registro como cabeceras, el separador decimal en forma de . y el separador de columnas ,. En ocasiones, nuestro fichero puede venir sin cabeceras por lo que deberemos importarlo indicando este echo y R se encargará de crearlas y nombrarlas automaticamente: datos&lt;-read.csv(&quot;./datos2.csv&quot;, header=FALSE, dec= &quot;.&quot;) En este último caso, si deseamos que en vez de los nombres de cabecera que crea R, tener unos personalizados, deberemos decírselo al realizar la importación: datos&lt;-read.csv(&quot;./datos2.csv&quot;, header=FALSE, dec= &quot;.&quot;, col.names = c(&quot;id&quot;, &quot;factor&quot;, &quot;volumen&quot;)) Para finalizar, nuestros ficheros pueden venir con valores nulos. Si los conocemos, deberemos indicarselo al comando read.csv mediante strings.na. Por ejemplo (todos los valores que en el fichero vengan con N/A los entederá como un valor nulo): datos&lt;-read.csv(&quot;./datos3.csv&quot;, header=TRUE, strings.na= &quot;N/A&quot;) Un añadido: en ocasiones los ficheros comienzan con líneas explicativas que no sirven para nada en el contexto de datos tabulares. Para saltar estas lineas podemos usar el comando skip= dentro de read.csv(). 1.4.3 Carga de datos usando el buscador En ocasiones, sobre todo si nuestro código lo compartimos con otros, es conveniente dejar la forma de entrada de nuestros datos a libre elección y que se localicen con el buscador del sistema. Para ello, haremos uso de la función file.choose(): filename &lt;- file.choose() data &lt;- read.csv(filename, header=TRUE, dec= &quot;.&quot;, sep=&quot;,&quot;) 1.4.3.1 Carga de datos desde una ruta web Generalmente nuestros ficheros estarán alojados en nuestro PC, pero en la actualidad, cada vez es más frecuente que los mismos estén alojados en un servidor de la web. La carga de estos no difiere en nada de la carga de un fichero ubicado en nuestro local. Para ello, en vez de una ruta de carpetas de nuestra computadora, pondremos la url donde se aloje el fichero y nos ayudaremos de la librería RCurl (Temple Lang 2021a) que facilita el trabajo de lectura y descarga: library(RCurl) datos_online &lt;- read.csv( text=getURL(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/parc.csv&quot;), header=TRUE) En ocasiones la librería RCurl (Temple Lang 2021a) falla (en Windows especialmente y/o si estamos antes de un proxy), por lo que podemos optar a la opción sencilla de descargar el fichero directamente: download.file(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/parc.csv&quot;, destfile = &quot;c:/temp/test.csv&quot;, method = &quot;curl&quot;) 1.4.3.2 Scrapping Una rutina bastante usual es descargar datos desde una web (a diferencia del anterior, no es un fichero alojado en una servidor). A este procedimiento se le denomina scrapping y usualmente deberemos comprender ficheros del tipo xmlo json (comprender, ya que no son estructuras rígidas como puede ser una tabla de un csv). En el caso de xml nos apoyaremos en la libreria XML (Temple Lang 2021b) el procedimiento consistirá en cargar la ruta en la que se aloja dicho elemento y a continuación indicarle a R cual es el nodo raíz para finalmente ver que datos hay encerrados entre las etiquetas &lt;&gt;. library(XML) library(RCurl) url&lt;-getURL(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/test.xml&quot;) xmldoc&lt;-xmlParse(url) rootnode&lt;-xmlRoot(xmldoc) rootnode[1] #De esta forma podremos ver el registro número 1 datos_raw&lt;-xmlSApply(rootnode, function(x) xmlSApply(x, xmlValue)) #Carga los datos fila a fila datos&lt;-data.frame(t(datos_raw), row.names =NULL) #Hacemos una transposición para aplicar correctamente las etiquetas como cabeceras Como sabemos HTML no deja de ser una formulación especial de xml y, por tanto, podremos usar lo anterior para extraer información de una web, web, hecha con lenguaje HTML url&lt;-getURL(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/poblacion.html&quot;) tables&lt;-readHTMLTable(url) #Nos extrae todas las tablas que hay en el fichero HTML tabla1&lt;-tables[[3]] #Extraerá la tabla 6 y se usan dos corchetes porque se busca en una lista de listas tabla2&lt;-readHTMLTable(url, which=3) #Idem al anterior pero no cargamos en local toda la web, ahorrando memoria Ahora hablemos de los ficheros json ficheros mucho más comunes para compartir datos y más eficientes en este sentido que xml. la librería que nos facilita el trabajo es jsonlite (Ooms 2020) library(jsonlite) library(RCurl) url&lt;-&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/currencies.json&quot; currencies&lt;-fromJSON(url) Si analizamos la estructura del fichero jsonveremos que como en el caso de xmles un conjunto de tados anidadados en etiquetas. Para poder extraer solo lo que nos interesa, podemos hacer uso del simbolo $ y crearnos un solo objeto con los datos que nos interese (es bastante util ir recorriendo la estructura según nos propone el predictor de RStudio a medida que vamos poniendo el símbolo $) datos&lt;-currencies$list$resources$resource$fields 1.4.3.3 El fichero Rdata R por defecto trabaja con un sistema de archivos propio, denominado con la extensión *.Rdata. Estos ficheros conservará todos los elementos tal y como los tengamos en la memoria una vez lo creemos, por lo que resultan bastante útiles para compartir ciertos elementos de nuestras sesiones que hayamos trabajado. cliente&lt;-c(&quot;Pepe&quot;, &quot;Juan&quot;, &quot;Julio&quot;) fecha&lt;-as.Date(c(&quot;2018/1/14&quot;, &quot;2018/12/26&quot;, &quot;2018/11/15&quot;)) cantidad&lt;-c(11.1, 16.18, 26.25) datos&lt;-data.frame(cliente, fecha, cantidad) save(datos, currencies, file = &quot;c:/Borrar/clientes.Rdata&quot;) load(&quot;c:/Borrar/clientes.Rdata&quot;) Existe otro formato de fichero en R, denominado rds que difiere del anterior en que este no almacena el nombre del objeto, por lo que cuando lo carguemos deberemos asignarle el nombre que queramos tener en nuestra sesión y, y esto es lo más importante, solo es capaz de guardar un solo objeto. Para guardar o cargar este fichero, usaremos los comandos loadRDS y saveRDS (en este último caso, guardando con la extensión *.rds). Finalmente, dentro de R podemos usar un comando muy interesante: save.image(file=\"\") el cual nos permite guardar en un fichero Rdata toda nuestra sesión, permitiéndonos así hacer portable nuestro trabajo. 1.4.4 Limpieza de la memoria en R A medida que vayamos trabajando en nuestro análisis, la memoria del sistema se irá llenando pudiendo llegar a abortar la sesión (depende la memoria RAM de nuestra computadora). Por ello, es recomendable ir eliminando aquellos objetos que no vayamos a utilizar nuevamente con el comando rm: rm(datos, tabla1) #Elimina el objeto datos y tabla1 rm(list=setdiff(ls(), c(&quot;cantidad&quot;, &quot;cliente&quot;))) #Elimina todos los objetos de la memoria menos rm(list=ls()) #Elimina todos los elementos de la memoria .rs.restartR() #Reinicia la sesión de R, eliminando cualquier resto que pudiera quedar 1.4.5 Control de versiones Para comenzar necesitaremos contar con el software Git en nuestro pc. Podemos hacerlo de dos formas: descargandolo e instalando la versión de nuestro sistema operativo o usando una versión portable. Desde aquí recomendamos https://www.syntevo.com/smartgit/download/, bien para usarlo o solo para utilizar el git portable. Una vez lo descomprimes vamos a RStudio y en Tools&gt;Global Options&gt;Git/SVN se le indica donde está git.exe, que está dentro de la instalación de git portable en la carpeta bin (generalmente en /git/bin/git.exe). A continuación abrimos el shell de git (/git/git-cmd.exe) y pondremos los siguientes comandos sustituyendo con nuestros datos lo que viene dentro de las comillas. git config --global user.name 'Your Name' git config --global user.email 'your@email.com' Crear un proyecto control de versiones es muy sencillo: creamos el proyecto habilitando el sistema de versioneado (checkbox Create a git repository dentro de File&gt;New Directory&gt;New Project). Para clonar un repositorio creamos un nuevo proyecto y le decimos que este sea con control de versiones (File&gt;Version Control) y ahí podremos introducir la dirección http del repositorio. También tendremos que indicarle donde lo guardaremos. Despues de ello, RStudio añadirá un botón, en la barra de herramientas principal, para gestionar el control de versiones (simbolizado con las letras GIT). Trabajar con un sistema de control de versiones daría para un libro completo sobre el tema por lo que no vamos a profundizar. Básicamente, nuestro trabajo se basará en realizar commits cada vez que deseemos fijar una foto de nuestros cambios y ejecutar pull/push de nuestros repositorios cuando queramos traernos los cambios a local o subirlos al servidor respectivamente. Indicar que en estas dos acciones, se abrirá un diálogo que nos solicita nuestro usuario y contraseña. Para profundizar en el tema es muy recomendable visitar el manual de Jenny Bryan Happy Git and GitHub for the useR y el video oficial de RStudio dentro de la serie RStudio Essentials 1.4.6 Buenas prácticas en la gestión de un proyecto Con frecuencia nos veremos envueltos en la gestión de proyectos de envergadura donde se mezclarán ficheros de datos creados y externos, scripts, elementos de consulta, documentación e incluso información temporal. Por ello, es recomendable ser disciplinado y crear una estructura de directorios adecuada. A continuación se dan unas ideas, adecuables a cada situación: Tratar los datos originales como de sólo lectura Limpiar los datos previamente, incluso fuera de R Usar herramientas de control de versiones como Git o SVN Empezar siempre nuestra sesión definiendo el directorio de trabajo y que este sea fijo Siempre trabajar con la misma estructura de directorios, aunque tengamos carpetas vacias. Por ejemplo, dentro de del directorio raíz de nuestro proyecto (cada proyecto contará con uno), crear las siguientes carpetas: bin: para guardar todos aquellos elementos accesorios para poder realizar los análisis (scripts de otros lenguajes, por ejemplo) data: donde almacenaremos todos los datos de inicio, metadata, doc: para poder guardar toda la documentación generada como wikis o manuales results: para guardar los datos limpios de nuestro análisis y los resultados src: para almacenaje de scripts creados temp: para almacenar los archivos temporales y prescindibles Escribir código legible y que este sea lo más pequeño posible (un elefante nos lo podemos comer a pequeños bocados) Siempre documentar tu proyecto, hasta lo más simple 1.5 Manejo de objetos en R 1.5.1 R como calculadora R es una potente calculadora pero quizás sea para lo que menos se usa. Su utilización es tan simple como escribir lo que queremos realizar (hagamos nuestro Hello, World sumando 1+1: 1+1 ## [1] 2 1.5.2 Manejo de variables Las variables, entendidas en R como en cualquier lenguaje de programación, son el núcleo sobre el que gira todo el sistema, si bien en este sistema se denominan objetos. Estas se asignan con los operadores &lt;- o = y con la función assing(), prefieriéndose el primero. a&lt;-1 a b=2 b c&lt;-d&lt;-3 c d assign(&quot;e&quot;, 13) e 3-&gt;f f Todas los objetos en R se realizan con objetos que son guardados en la memoria activa del ordenador, sin usar archivos temporales, por lo que deberemos tener muy en cuenta la RAM de nuestra máquina. Es importante destacar que si un objeto existe con una valor, si le asignamos otros, el primero cambia irremediablente: a a&lt;-45 a Por otro lado, el valor asignado a un objeto puede ser el resultado de una operacion y/o de una función: n &lt;- 5 + rnorm(1) En ocasiones, y para abreviar, podremos encontrarnos con comandos en una sola línea: si estos están separados con ; se ejecutarán de forma independiente: a&lt;-1; b&lt;-2; a; b ## [1] 1 ## [1] 2 A veces podremos querer listar los objetos que existen en memoria, para lo cual usaremos ls(): ls() ## [1] &quot;a&quot; &quot;b&quot; ls(pat = &quot;m&quot;) # Lista aquellos objetos que contengan una m ## character(0) 1.6 Manejo de datos en R 1.6.1 Tipos de datos El dato en R es un concepto distinto al que estamos acostumbrados a tratar en Estadística Aplicada y se debe entender como el resultado de ejecutar una determinada expresión, es decir, es un objeto. En otros, los tipos de datos (objetos) fundamentales de R son: Vectores. Conjunto de elementos en un determinado orden. Factores. Vector donde sus elementos provienen de un número finito de categorías. Matrices. Estructura de datos bidimensional de valores de igual tipo Listas. Estructura de datos más amplia que y puede contener colecciones arbitrarias de datos data.frame. Estructura de datos bidimensional de elementos, cuyas columnas pueden estar formadas por elementos de distinto tipo Como vemos unos se diferencian de otros en función de los tipos de elementos que contienen. Estos elementos poseen distintos atributos, pero de ellos, para nuestros propósitos hemos de destacar el modo. En concreto, en R, distinguimos cuatro modos: Lógico o modo binario, verdadero o falso (T o F, respectivamente) Númerico, donde los valores posibles son números reales Complejo, donde los valores posibles son números complejos Carácter, donde los valores son caracteres (separados por comillas) Fechas, definidas en formato 2015-06-25 o 2015-06-25 15:15 1.6.1.1 Vectores Es uno de los tipos de datos más utilizados. Como regla ineludible es que sus elementos sean del mismo modo. Su creación se realiza a través de la función c (del inglés concatenate), siguiendo la siguiente estructura: x&lt;-c(118, 160, 64, 138, 168, 140, 109, 135, 220, 180, 151, 129, 117, 121, 86, 170, 62, 104, 184) z&lt;-c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;c&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) w&lt;-c(5:-7) w ## [1] 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 -7 El trabajo con objetos de R tiene grandes ventajas. Una de ellas es el crear datos a partir de otros o ejecutar operaciones con ellos. x1&lt;-c(118, 160, 64, 138, 168) x2&lt;-c(140, 109, 135, 220, 180) x3&lt;-c(x1, 140, 109, 135, 220, 180, 151, 129, 117, 121, 86, 170, 62, 104, 184) x1*x2 ## [1] 16520 17440 8640 30360 30240 (x1^2)/(x2) ## [1] 99.45714 234.86239 30.34074 86.56364 156.80000 Si, deseamos saber las características del vector creado, podemos usar funciones como las siguientes: mode(x) ## [1] &quot;numeric&quot; length(x) ## [1] 19 Otras funciones útiles son las que se enumeran a continuación: is.numeric(x) #Pregunta si es numérico ## [1] TRUE class(x) #Indica el tipo de valor ## [1] &quot;numeric&quot; nchar(z) # Cuenta el número de caracteres ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1.6.1.2 Factores Los factores son uno de los tipos de datos más importantes en Estadística Aplicada. Su definición en R es muy sencilla y se hace con la instrucción siguiente (nos apoyaremos en el vector z, anteriormente creado): f&lt;-as.factor(z) Existen comandos asociados a los factores bastante útiles. De ellos destaca levels, que nos indica los niveles del factor: levels(f) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; 1.6.2 Valores perdidos y nulos El tratamiento de estos tipos de valores es muy simple en R. Los valores perdidos se definen como NA y a diferencia de un valor nulo,NULL, implica la existencia de valor. Esto se puede ver fácilmente con el siguiente comando: s&lt;-c(1, NA, 3, 4) r&lt;-c(1, NULL, 3, 4) s ## [1] 1 NA 3 4 r ## [1] 1 3 4 length(s) ## [1] 4 length(r) ## [1] 3 Si contamos con valores perdidos, es de vital importancia no tratarlos en nuestros análisis. Para ello, usaremos el comando na.rm=T: mean(s) ## [1] NA mean(s, na.rm=T) ## [1] 2.666667 Para finalizar, en R, hemos de evitar confundir NA con NaN. Mientras el primero, como hemops indicado responde a la inexitencia de valor, el segundo representa las siglas Not a Number, siendo arrojado por el porgrama cuando el resultado de un determinado cálculo así lo sea. sqrt(-23) ## Warning in sqrt(-23): Se han producido NaNs ## [1] NaN 0/0 ## [1] NaN 1.6.2.1 Gestión de valores NA Cuando cargamos un fichero de datos que contiene datos vacios, podremos transformarlos a NA usando el comando na.strings=\"\" poniendo dentro de las comillas el valor que represente ese dato vacio (si efectivamente el vacio lo es, lo podremos indicar no poniendo nada dentro de estas comillas). download.file(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/parcna.txt&quot;, destfile = paste(tempdir(), &quot;\\\\parcna.txt&quot;, sep=&quot;&quot;), method = &quot;curl&quot;, overwrite=TRUE) data &lt;- read.table(paste(tempdir(), &quot;\\\\parcna.txt&quot;, sep=&quot;&quot;), na.strings=&quot;na&quot;, header=TRUE) En ocasiones podríamos querer despreciar esos casos que contienen valores NA: lo haremos facilmente con la sentencia na.omit(). data_clean&lt;-na.omit(data) #Observemos que se ha reducido el número de registros respecto al objeto data Hagamos algo más complejo: quedemosnos con aquellos registros donde no existan NA en un determinado campo myfield (usaremos ! que es el indicador de negación, el concepto is.na y los corchetes para señalizar filas y/o columnas). new_data&lt;-data[!is.na(data$COMARCA),] Parecido a lo anterior, si quisieramos quedarnos solo con los registros completos, es decir, con solo aquellos que no tienen NAen ninguna columna, podríamos hacer: new_data.2&lt;-data[complete.cases(data),] Finalmente, si quisieramos convertir un determinado valor a NA podríamos ejecutar: data$AB[data$HD==0]&lt;-NA 1.6.2.1.1 Reemplazar valores NA con parámetros de la muestra Como se ha dicho anteriormente, al calcular parámetros de nuestra muestra y contar con valores NA el resultado será NA salvo que le indiquemos que no lo tenga en cuenta con na.rm. De esta forma, evitaremos el error, pero, por contra, perderemos registros de nuestro data.frame si este cuenta con más. Por ello, en ocasiones es adecuado reemplazar esos valores con el parámetro que estemos calculando: data$NC&lt;-ifelse(is.na(data$NC), mean(data$NC, na.rm = TRUE), data$NC) Esta opción es adecuada para variables continuas. En el caso de variables categóricas, podemos asignar un valor aleatorio, de todos los posibles con los que contamos en ese caso. rand.impute&lt;-function(x){ # x es un vector de datos que puede contener NA missing&lt;-is.na(x) # missing es un vector que contiene TRUE o FALSE en función de que x sea NA o no n.missing&lt;-sum(missing) #sumamos missing y obtenemos el número de verdaderos y por tanto obtenemos cuantos NA tenemos x.obs&lt;-x[!missing] # Obtenemos que datos tienen valor diferente de NA en x, es decir, me quedo con los que no son NA imputed&lt;-x imputed[missing]&lt;-sample(x.obs, n.missing, replace = TRUE) #Extraemos una muestra aleatoria de los datos que conocemos, x.obs, de un tamaño n.missing, y los asignamos a los que no conocemos return (imputed) } random.impute.dataframe&lt;- function(dataframe, cols) { names&lt;-names(dataframe) for (col in cols){ name&lt;-paste(names[col], &quot;.imputed&quot;, sep=&quot;&quot;) dataframe[name]=rand.impute(dataframe[,col]) } } data2&lt;-data random.impute.dataframe(data2,c(1,3)) 1.6.2.1.2 Eliminar registros NA Las más de las veces, querremos eliminar los datos NA. Para ello, haremos uso del comando na.omit: data_na.omit&lt;-na.omit(data) 1.6.3 Registros duplicados La gestión de los registros duplicados es muy sencilla en R con la función unique() data2&lt;-unique(data) 1.6.4 Reescalado Para que funcionen mejor muchos algoritmos, hay que normalizar las variables de entrada al algoritmo. Por Normalizar conocemos la técnica que nos permite comprimir o extender los valores de la variable para que estén en un rango definido. Asímismo, es bastante interesante usar esta técnica con variables con dispersiones muy grandes (es decir con valores muy elevados y muy pequeños). El trabajo de normalización se puede realizar facilmente con la libreria scales (Wickham and Seidel 2020) library(scales) data$VSC.rescaled&lt;-rescale(data$VSC) #El valor más pequeño tomará el valor cero y el más alto tomará el valor uno. El resto se escala de forma líneal: data$VSC.rescaled.manual&lt;-(data$VSC-min(data$VSC))/(max(data$VSC)-min(data$VSC)) data$VSC.rescaled.100&lt;-rescale(data$VSC, to=c(0,100)) En el caso que quisieramos reescalar varias columnas, podríamos hacer uso de un bucle: rescale.all&lt;-function(dataframe, cols){ names&lt;-names(dataframe) for (col in cols){ name&lt;-paste(names[col], &quot;rescaled&quot;, sep=&quot;.&quot;) dataframe[name]&lt;-rescale(dataframe[,col]) } dataframe } data2&lt;-data data2&lt;-rescale.all(data2, c(4,5)) 1.6.5 Normalizado y estandarizado Las más de las veces realizamos pruebas, modelos o simplemente estimamos valores de parámetros en los que es obligatoria la existencia de normalidad en nuestros datos. Se entiende por variable normal aquella cuya distribución de probabilidad se ajusta una curva acampanada en la que la media aritmética, la mediana y la moda de la distribución son iguales y se localizan en el pico. Así, la mitad del área bajo la curva se encuentra a la derecha de este punto central y la otra mitad está a la izquierda de dicho punto. No es la única función de distribución, pero si es la más común y una de sus características más importantes es que casi cualquier distribución de probabilidad, tanto discreta como continua, se puede aproximar por una normal bajo ciertas condiciones. Por ello, la mayoría de los estdísticos más rutinarios exigen la existencia de normalidad para validar las hipótesis. El análisis de la normalidad se puede hacer de una forma rudimentaria comprobando si la media, mediana y moda son iguales o de una forma más compleja a raíz de pruebas de hipótesis como la de Kolmogorov-Smirnov o el test de Shapiro. Asimismo, gráficamente, representando el histograma de frecuencias o con gráficos de cajas (box-plot) podremos comprobar su cumplimiento. En R, estas pruebas forman parte del núcleo del software, usándose los siguientes comandos para comprobarla: mean(data$VSC) # Media aritmética ## [1] 30.66169 median(data$VSC) # Mediana ## [1] 27.895 shapiro.test(data$VSC) #Test de Shapiro (para pruebas con más de 50 datos) ## ## Shapiro-Wilk normality test ## ## data: data$VSC ## W = 0.93232, p-value = 3.935e-16 ks.test(data$VSC, &quot;pnorm&quot;, mean(data$VSC), sd(data$VSC)) # Test de Kolmogorov ## Warning in ks.test(data$VSC, &quot;pnorm&quot;, mean(data$VSC), sd(data$VSC)): ties should ## not be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: data$VSC ## D = 0.066132, p-value = 0.008827 ## alternative hypothesis: two-sided par(mfrow=c(2,2)) # Representación gráfica plot(data$VSC) hist(data$VSC) boxplot(data$VSC) qqnorm(data$VSC) par(mfrow=c(1,1)) Muchas veces la no existencia de normalidad puede ser solucionada con una transformación de los datos. La transformación consiste en aplicar una determinada operación matemática, de forma que una vez aplicada la variable alcance cierto grado de normalidad. En general con variables cuya distribución esté sesgada a la izquierda, la normalidad se puede alcanzar con aplicando el logaritmo neperiano de la misma (así comprimiremos la cola de la izquierda y extenderemos la de la derecha.). Si, por el contrario, la distribución está sesgada a la derecha, la aplicación de una raíz cuadrada puede solucionar el problema. Otras veces, el alcanzar la normalidad no es tan sencillo y es necesario utilizar transformación en las que entran en juego potencias. En este sentido destaca la conocida tranformación planteada por Box y Cox. En R su consecución se realiza a partir de la aplicación de librería car (Fox, Weisberg, and Price 2021) library(car) summary(powerTransform(data$VSC)) ## bcPower Transformation to Normality ## Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd ## data$VSC 0.4838 0.5 0.4059 0.5616 ## ## Likelihood ratio test that transformation parameter is equal to 0 ## (log transformation) ## LRT df pval ## LR test, lambda = (0) 177.8797 1 &lt; 2.22e-16 ## ## Likelihood ratio test that no transformation is needed ## LRT df pval ## LR test, lambda = (1) 144.6436 1 &lt; 2.22e-16 La variable transformada será (-1+datos^\"Est.Power\")/(\"Est.Power\"): data$VSC.transform&lt;-(-1+data$VSC^0.4838)/(0.4838) par(mfrow=c(2,2)) # Representación gráfica plot(data$VSC.transform) hist(data$VSC.transform) boxplot(data$VSC.transform) qqnorm(data$VSC.transform) par(mfrow=c(1,1)) Finalmente, en ocasiones nos interesará estandarizar (o tipificar) nuestras variables, esto es, que nuestra variable se convierta en otra con una distribución de media cero y desviación típica 1. Esto se consigue facilmente con el comando scale(). data$VSC.tip&lt;-scale(data$VSC) mean(data$VSC.tip) ## [1] 8.456093e-17 sd(data$VSC.tip) ## [1] 1 1.6.6 Categorizar los datos Muchas veces nos interesará categorizar los datos en rangos en función a un valor numérico. Para ellos, haremos uso de la función cut: cortes.num&lt;-c(-Inf, 16, 25, 40, Inf) cortes.label&lt;-c(&quot;bajo&quot;, &quot;medio&quot;, &quot;alto&quot;, &quot;muy alto&quot;) data$VSC.cat&lt;-cut(data$VSC, breaks = cortes.num, labels = cortes.label) Siguiendo con lo anterior, al realizar ciertos análisis como los de regresión, la variable dependiente y las independientes no solamente pueden estar dadas por variables cuantitativas: existen otros tipos de variables de carácter cualitativo. Dichas variables se conocen comúnmente como variables dummy y usualmente, dichas variables indican la presencia o ausencia de una cualidad o atributo, tomando valor de 1 en una submuestra y 0 en el resto de la muestra. La gestión de este tipo de variables podemos hacerlo con la libreria dummies (Brown 2012). library(dummies) data.dummy&lt;-dummy.data.frame(data, sep=&quot;.&quot;) head(data.dummy) ## COMARCA.ANDEVALO COMARCA.COSTA COMARCA.SIERRA COMARCA.NA NC HD AB ## 1 0 1 0 0 683.33 16.82 8.00 ## 2 0 1 0 0 800.00 16.15 6.35 ## 3 0 1 0 0 800.00 16.15 6.35 ## 4 0 1 0 0 800.00 16.15 6.35 ## 5 0 1 0 0 650.00 18.48 9.33 ## 6 0 1 0 0 750.00 16.70 9.23 ## VSC VSC.rescaled VSC.rescaled.manual VSC.rescaled.100 VSC.transform ## 1 41.85 0.2771390 0.2771390 27.71390 10.519687 ## 2 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 3 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 4 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 5 53.77 0.3568182 0.3568182 35.68182 12.142223 ## 6 49.64 0.3292112 0.3292112 32.92112 11.603315 ## VSC.tip VSC.cat.bajo VSC.cat.medio VSC.cat.alto VSC.cat.muy alto ## 1 0.556328224 0 0 0 1 ## 2 0.004390957 0 0 1 0 ## 3 0.004390957 0 0 1 0 ## 4 0.004390957 0 0 1 0 ## 5 1.149039236 0 0 0 1 ## 6 0.943678793 0 0 0 1 data.dummy2&lt;-dummy.data.frame(data, names=c(&quot;COMARCA&quot;), sep=&quot;.&quot;) # Solo lo aplicamos a una o varias variables en vez de a todas las categóricas del data.frame head(data.dummy2) ## COMARCA.ANDEVALO COMARCA.COSTA COMARCA.SIERRA COMARCA.NA NC HD AB ## 1 0 1 0 0 683.33 16.82 8.00 ## 2 0 1 0 0 800.00 16.15 6.35 ## 3 0 1 0 0 800.00 16.15 6.35 ## 4 0 1 0 0 800.00 16.15 6.35 ## 5 0 1 0 0 650.00 18.48 9.33 ## 6 0 1 0 0 750.00 16.70 9.23 ## VSC VSC.rescaled VSC.rescaled.manual VSC.rescaled.100 VSC.transform ## 1 41.85 0.2771390 0.2771390 27.71390 10.519687 ## 2 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 3 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 4 30.75 0.2029412 0.2029412 20.29412 8.776134 ## 5 53.77 0.3568182 0.3568182 35.68182 12.142223 ## 6 49.64 0.3292112 0.3292112 32.92112 11.603315 ## VSC.tip VSC.cat ## 1 0.556328224 muy alto ## 2 0.004390957 alto ## 3 0.004390957 alto ## 4 0.004390957 alto ## 5 1.149039236 muy alto ## 6 0.943678793 muy alto Finalmente, la eliminación de un objeto se debe realizar con el comando rm: rm(a) 1.7 Profundizando en el data.frame 1.7.1 Introducción Las más de las veces nuestras estructuras de datos se enmarcarán dentro de tablas clásicas y son necesiarias para la amplia mayoría de análisis estadísticos.Estas tablas, en R, se denominan data.frame y, lo más común es que partamos de una tabla con datos y con ella y sobre ella ejecutemos nuestros análisis. R, como tal, no dispone de un sistema visual para gestionar las tablas por lo que deberemos conocer los rudimentos para procesarlas y adaptarlas a nuestros requerimientos una vez las tengamos en la memoria despues de haberlas cargado tal y como se explicó en el capítulo ??. 1.7.2 Visualizar tablas Generalmente comenzaremos visualizando nuestra tabla. Para ello, R dispone, entre otros de los sigueintes comandos (comenzaremos cargando el fichero iris): iris plot(iris) # hace una representación gráfica summary(iris) #calcula los estadísticos descriptivos head(iris) #extrae los 6 primeros registros tail (iris) #extrae los 6 últimos registros names(iris) # muestra las cabeceras de las columnas dim(iris) # visualiza las dimensiones de filas y columnas de la tabla 1.7.3 Extracción de datos en nuevos data.frame Un recurso muy interesante de R es como extrae los datos del data.frame. Veamos algunos ejemplos: iris[1:10,] #extrae las 10 primeras filas iris[,1:3] #extrae las 3 primeras columnas iris[1:10,1:3] #extrae las 10 primeras filas y de las 3 primeras columnas iris[c(2,5,8),] #extrae las filas 2, 5, y 8 iris[, c(1,3)] #extrae las columnas 1 y 3 Por otro lado, si no conocemos el número de columna del campo que deseamos, siempre podremos, evidentemente, extraerlo con su nombre: iris[, &quot;Sepal.Width&quot;] Finalmente, si deseamos extraer determinados elementos con expresiones lógicas, podremos hacerlo con un simple comando: iris[iris$Species == &quot;setosa&quot;,] 1.7.4 Gestionar los data.frame Si antes nos referiamos a la visualización y extracción de los elementos, veamos ahora como se gestionan los data.frameen su conjunto. Para añadir una columna, lo haremos añadiendo una variable dentro de ella: datos&lt;-iris datos$Petal.Area &lt;- datos$Petal.Length * datos$Petal.Width head (datos) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Area ## 1 5.1 3.5 1.4 0.2 setosa 0.28 ## 2 4.9 3.0 1.4 0.2 setosa 0.28 ## 3 4.7 3.2 1.3 0.2 setosa 0.26 ## 4 4.6 3.1 1.5 0.2 setosa 0.30 ## 5 5.0 3.6 1.4 0.2 setosa 0.28 ## 6 5.4 3.9 1.7 0.4 setosa 0.68 Y para eliminarlas, lo podremos hacer anulando la columna deseada: datos$Petal.Area &lt;- NULL head (datos) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Por otro lado, podremos desear ordenar los datos de una determinada manera.Para ello, haremos uso de order: datos2 &lt;- iris[order(iris$Sepal.Width),] 1.7.5 La libreria dplyr En este punto, si bien como se ha descrito en los puntos anteriors R, en su versión base es bastante potente en la gestión de los data.frame, es adecuado destacar el paquete dplyr (Wickham, François, et al. 2021)que entre otras cosas, hace más natural la gramática de trabajo con las tablas. La sintaxis grámatica de esta libreria facilita la gestión de los elementos ya que las peticiones y comandos se hacen con las palabras habituales en otros softwares, lo que nos descarga en gran medida de memorizar comandos recurrentes. Veamos algunos ejemplos: library (dplyr) datos&lt;-iris subset &lt;- select(datos, Sepal.Width:Species) #extrae las columnas que van de Sepal.Width:Species Petal.Length.5&lt;- filter(datos, Petal.Length &gt; 5) #extrae aquellas filas donde Petal.Length es mayor de cinco Petal.Length.order &lt;- arrange(datos, desc(Petal.Length)) #Extrae un nuevo data.frame ordenado por Petal.Length de forma descendente Petal.Length.rename&lt;-rename(iris, Long.Petalos=Petal.Length, Ancho.Petalos=Petal.Width) #Renombra determinadas columnas iris.Petal.Area &lt;- mutate(iris, Petal.Area = Petal.Width*Petal.Length) #Añade una nueva variable al data.frame iris.aleat&lt;-sample_n(iris, 4) #Extrae aleatoriamente cuatro registros iris.aleat.025&lt;- sample_frac(iris, 0.25, rep=TRUE) #Extrae un 25% de obs con reemplazamiento Mención aparte requiere el operador %&gt;%. Este permite concatenar varias secuencias de comandos aliviando drásticamente nuestro código: iris.2&lt;- select(iris, contains(&#39;Petal&#39;)) iris.3&lt;- filter(iris.2, Petal.Length &gt; 4) iris.4&lt;- arrange(iris.3, Petal.Length) head(iris.4) ## Petal.Length Petal.Width ## 1 4.1 1.0 ## 2 4.1 1.3 ## 3 4.1 1.3 ## 4 4.2 1.5 ## 5 4.2 1.3 ## 6 4.2 1.2 iris.5&lt;-iris %&gt;% select(contains(&#39;Petal&#39;)) %&gt;% filter(Petal.Length &gt; 4) %&gt;% arrange(Petal.Length) head(iris.5) ## Petal.Length Petal.Width ## 1 4.1 1.0 ## 2 4.1 1.3 ## 3 4.1 1.3 ## 4 4.2 1.5 ## 5 4.2 1.3 ## 6 4.2 1.2 Para finalizar, queda mencionar que la concatenación de los comandos de dplyr con el operador %&gt;%permite crear estructuras no definidas en el paquete, pero que son de uso recurrente. Veamos como calculamos unos subtotales de nuestra tabla irispor especie: iris.summarize&lt;-iris %&gt;% group_by(Species) %&gt;% summarise(mean(Petal.Length))%&gt;% rename(Especies=Species, Long.Petalos.media=`mean(Petal.Length)`) #Notese las comillas en el campo mean(Petal.Length) que no está en el campo Species 1.8 Estructuras de control 1.8.1 Introducción Las estructuras de control en R nos permiten automatizar los flujos de programación y las secuencias de comandos, haciendo que trabajos reiterativos sena más sencillos y abordables (no confundir con las funciones, que se abordarán en el capítulo ??). Las estructuras de control más utilizadas son: if / else: el fragmenteo se ejecutará o no en función de una condición. for: se ejecuta un bucle de forma definida según un número determinado de ocasiones. while: ejecuta un bucle mientras sea verdadera una condición. Junto a ellas, debemos tener presentes otras de uso menor como: repeat: el bucle se ejecutará indefinidamente hasta que lo detengamos con break. next: salta a la siguiente ejecución de un bucle. ###if/else Como se ha dicho, esta estructura de control se ejecutara o no, en función de que se cumpla una determinada condición. De forma esquemática: if(&lt;condicion&gt;) { ## bloque de código } O, en el caso de que queramos que se ejecute algo distinto a que condición nos marca, podremos hacer: if(&lt;condicion&gt;) { ## bloque de código } else { ## otro bloque de código } Tambien podremos anidarlos: if(&lt;condicion1&gt;) { ## bloque de código } else if(&lt;condicion2&gt;) { ## otro bloque de código } else { ## otro bloque de código } Veamos un ejemplo muy simple: x &lt;- runif(1, 0, 10) if(x &gt; 3) { y &lt;- TRUE } else { y &lt;- FALSE } El anterior comando genera un objeto x cuyo valor se obtiene de forma aleatoria entre 0 y 10. A continaución hacemos nuestro bucle: si x es mayor de 3, se creará un objeto y con valor TRUE, mientras que si no es así, y tomará el valor FALSE. 1.8.2 while A diferencia de el anterior con while comemza comprobando una condición: si esta es verdadera, entonces se comienza con el bucle. Una vez completada una ejecución de este bloque, se comprueba la condición nuevamente y así sucesivamente hasta que la comprobación de la condición de falso. Veamos su forma: while(&lt;condicion&gt;) { # código } A modo de ejemplo: i &lt;- 1 while (i &lt; 6) { print(i) i = i+1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 1.8.3 for Los bucles for son, con diferencia y por su versatilidad, los más utilizados en R. En este caso, estos toman el valor que se asigna a partir de los elementos de un objeto (generalmente listas) y, de forma sucesiva, van ejecutando los ciclos. Esquemáticamente: for(&lt;variable&gt; in &lt;objeto iterable&gt;) { # código a ejecutar } Por ejemplo: for(i in 1:5){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Finalmente indicar que, como el anterior, podremos anidar unos dentro de otros. 1.9 Los gráficos en R Si por algo destaca R es por su capacidad gráfica y por la flexibilidad en la creación de los mismos ofreciendo una increíble variedad de gráficos (Para tener una idea, escribe demo(graphics)). De antemano se debe tener claro una cuestión para el aspecto gráfico en R: el resultado de una función gráfica no puede ser asignado a un objeto (recordemos que el resultado de una operación de análisis estadístico se podia asignar a un objeto) si no que se muestra en el motor gráfico y nada mas. Existen dos tipos fundamentales de funciones gráficas: las funciones gráficas de alto nivel que crean una nueva gráfica y las funciones gráficas de bajo nivel que agregan elementos a una gráfica ya existente. La instalación base de R trae una serie de gráficos que a buen seguro son lo suficientemente versátiles para cumplir con todas nuestras expectativas. Pero si con ellos no fuera suficiente, existen paquetes adaptados a esta función que amplían la riqueza gráfica del sistema R. La función usual de R para producir un gráfico es el comando plot: datos&lt;-matrix(c(2,5,1,5,8,2,1,9,5,1,5,9), ncol=2) colnames(datos)&lt;-c(&quot;Peso&quot;, &quot;Estatura&quot;) datos ## Peso Estatura ## [1,] 2 1 ## [2,] 5 9 ## [3,] 1 5 ## [4,] 5 1 ## [5,] 8 5 ## [6,] 2 9 plot(datos) Si quisiéramos añadir una descripción a nuestro gráfico, podríamos usar el comando main que pondrá un título al mismo. Además, podemos detallar un poco más las descripciones de los ejes con xlab e ylab. Obviamente, también podremos cambiar el color y el tipo de punto usando col y type. Para finalizar, podríamos añadir una rejilla en las marcas principales con grid y añadir un texto descriptivo con text en una determinada coordenada. Veámoslos todos juntos: plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;) grid() text(7, 8, &quot;texto explicativo&quot;) Seguimos ampliando las opiones de nuestro gráfico. En ocasiones podremos desear añadir una determinada línea al mismo. para ello, nos podemos servir de la función abline: plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;, type=&quot;p&quot;) abline(h=4, col=&quot;green&quot;) # Añade línea horizontal abline(v=6, col= &quot;blue&quot;) # Análogo para línea vertical abline(1,4, col=&quot;grey&quot;, lty=2) # Añade una línea de pendiente 1 y que corta al origen en 3 con trazo discontinuo El sistema tradicional de gráficos ofrece una variedad de tipos de gráficos básicos: la función plot() produce gráficos de dispersión, la función barplot() produce gráficos de barra, la función hist() produce histogramas, la función boxplot() produce gráficos de caja, y la función pie() produce gráficos de tarta. Hemos de tener presente que R no distingue más tipos de gráficos siendo otros tipos de gráficos variaciones de estos principales. Por ello, un gráfico de dispersión con sus puntos interconectados por una línea, es una nueva orden en el comando plot(), denominada type: plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;, type=&quot;p&quot;) plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;, type=&quot;l&quot;) plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;, type=&quot;b&quot;) plot(datos, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Peso (kg)&quot;, ylab=&quot;Estatura (m)&quot;, col=&quot;red&quot;, type=&quot;h&quot;) O, repeticiones más o menos complejas del gráfico principal como el socorrido pairs() para ver correlaciones: data(iris) names (iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; pairs(iris[1:3], pch = 21) Llegados a este punto, llega el momento de explicar un recurso del motor gráfico de R: la organización de los gráficos en la ventana. Para ello se utiliza la función par() que controla parámetros gráficos adicionales. veamos como funciona: par(mfrow=c(2,1)) # Dibuja una matriz de gráficos 2x1: un gráfico debajo de otro par(mfrow=c(2,3)) # Matriz de gráficos 2 x 3 : dos filas por tres columnas par(mfrow=c(1,1)) # un solo gráfico por ventana: la opción por defecto Un detalle más. Ciertos elementos tienen gráficos predefinidos, ya que se sobreentiende que son de obligado diseño. Caso de esto pueden ser los gráficos descriptivos de un modelo lineal, por ejemplo (olvidémonos de la primera sintaxis, que se verá en capítulos más adelante): download.file(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/parc.csv&quot;, destfile = paste(tempdir(), &quot;\\\\parc.csv&quot;, sep=&quot;&quot;), method = &quot;curl&quot;, overwrite=TRUE) data &lt;- read.csv(paste(tempdir(), &quot;\\\\parc.csv&quot;, sep=&quot;&quot;),header=TRUE) datos.df&lt;-data.frame(data) model&lt;-lm(VSC~HD, datos.df) par(mfrow=c(2,2)) # De esta forma creamos una estructura gráfica de 2x2 plot(model) par(mfrow=c(1,1)) # Y ahora volvemos a poner el sistema gráfico en su posición original Una última cuestión. Hasta ahora hemos estado trabajando con un conjunto de datos sencillo, compuesto por dos variables. En el caso de disponer de más y querer representarlas, deberemos modificar nuestra sentencia para indicar este hecho: plot(datos.df$VSC~datos.df$HD, main=&quot;Mi primer gráfico en R&quot;, xlab=&quot;Volumen (m3)&quot;, ylab=&quot;Altura (m)&quot;, col=&quot;red&quot;, type=&quot;p&quot;) Como vemos las posibilidades gráficas de R son muy grandes y son objeto de monografías exclusivas sobre sus capacidades. 1.9.1 ggplot2 La librería ggplot2 (Wickham, Chang, et al. 2021) es un paquete que se ha convertido en un imprescindible dentro del sistema R por sus capacidades gráficas. Si bien cambia un poco la concepción que hasta ahora tenemos de los gráficos, los beneficios que nos reporta suplen con creces su aparente complejidad. Este paquete se basa en elaborar un gráfico a partir de un proceso de acumulación de capas o layers siendo su esquema constante según la siguiente expresión: ggplot(data.frame, aes(x = variable)) + geom_forma() + stat_valor donde: - data.frame es nuestro objeto de datos (en formato data.frame), - aes() controla la estética del gráfico, es decir los elementos representables gráficamente (la posición x e y, el color,las columnas de la tabla de datos) - geom_ lo define con puntos, rectas, histogramas, densidades, etc. (además es capaz de superponer distintas geometrías), - y stat_(), que es un resumen estdístico de los datos asociado al tipo de geometría con que trabajamos. Veamos un ejemplo: download.file(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/Pesadas.txt&quot;, destfile = paste(tempdir(), &quot;\\\\Pesadas.txt&quot;, sep=&quot;&quot;), method = &quot;curl&quot;, overwrite=TRUE) arbol &lt;- read.csv(paste(tempdir(), &quot;\\\\Pesadas.txt&quot;, sep=&quot;&quot;),sep=&quot;\\t&quot;, header=TRUE) arbol.df&lt;-data.frame(arbol) names(arbol.df) ## [1] &quot;CLM&quot; &quot;CH&quot; &quot;Clonal&quot; &quot;Edad&quot; &quot;Arbol&quot; &quot;Nb&quot; &quot;Ht&quot; &quot;Dn&quot; ## [9] &quot;Htmax&quot; &quot;P&quot; &quot;Pf&quot; &quot;Ph&quot; library(ggplot2) ggplot(arbol.df, aes(x =CLM)) + geom_bar() Como vemos, le hemos indicado que utilice el conjunto de datos arbol.df, que use en el eje horizontal los valores de CLM (al no incluir datos en la vertical, realizará un conteo) y que el tipo de gráfico sea de barras. En este punto, ggplot2 es muy versatil definiendo el tipo de goemetría (o gráfico) aceptando multitud de variaciones: geom_bar, geom_point,geom_boxplot, geom_violin, geom_line, Una de las ventajas del uso de ggplot2 es que podemos guardar el gráfico en un objeto y añadir más capas. Por ejemplo, añadamos las etiquetas de los ejes y el título: grafico&lt;-ggplot(arbol.df, aes(x = CLM)) + geom_bar() grafico+xlab(&quot;Monte&quot;) + ylab (&quot;Número de individuos&quot;)+ggtitle(&quot;Gráfico de individuos&quot;) Ahora vamos a controlar la estética del gráfico, haciendo que las barras sean más pequeñas y cambien de color: grafico&lt;-ggplot(arbol.df, aes(x = CLM)) + geom_bar(width=0.5, colour=&quot;black&quot;, fill=&quot;red&quot;) grafico+xlab(&quot;Monte&quot;) + ylab (&quot;Número de individuos&quot;)+ggtitle(&quot;Gráfico de individuos&quot;) Los temas (theme) son un conjunto de opciones predefinidas sobre la apariencia de los objetos en ggplot. El tema por defecto del ggplot dibuja el gráfico sobre un fondo gris. Podemos cambiarlo a otro tema añadiendo el comando especifico. En este caso usamos el theme_bw() (es interesante el paquete ggthemes que incorpora una gran variedad de ellos): grafico+theme_bw() Ahora vamos a ir un paso más allá. Vamos a crear un grafico de dispersión, comparando dos variables y usaremos el comando facet que permite reproducir el mismo gráfico en diferentes niveles de un factor: #Version nº1 grafico&lt;-ggplot(arbol.df, aes(x = Ht, y= Dn)) + geom_point(width=0.5, colour=&quot;black&quot;, fill=&quot;red&quot;) ## Warning: Ignoring unknown parameters: width grafico+xlab(&quot;Altura&quot;) + ylab (&quot;Diametro&quot;)+ggtitle(&quot;Gráfico de individuos&quot;)+ facet_grid(CLM ~.) #Version nº2 grafico&lt;-ggplot(arbol.df, aes(x = Ht, y= Dn)) + geom_point(width=0.5, colour=&quot;black&quot;, fill=&quot;red&quot;) ## Warning: Ignoring unknown parameters: width grafico+xlab(&quot;Altura&quot;) + ylab (&quot;Diametro&quot;)+ggtitle(&quot;Gráfico de individuos&quot;)+ facet_grid(.~ CLM) ¿Y si quisiéramos rotar los ejes para observar mejor nuestros datos? Para ello podemos usar coord_flip(). grafico+xlab(&quot;log de la altura&quot;) + ylab (&quot;Número de individuos&quot;)+ggtitle(&quot;Gráfico de individuos&quot;)+ facet_grid(.~ CLM)+coord_flip() Para finalizar, veamos la opción ´stat´ que como dijimos es un resumen estadístico de los datos asociado al tipo de geometría con que trabajamos: grafico+xlab(&quot;Altura&quot;) + ylab (&quot;Diametro&quot;)+ggtitle(&quot;Gráfico de individuos&quot;)+ stat_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; grafico+xlab(&quot;Altura&quot;) + ylab (&quot;Diametro&quot;)+ggtitle(&quot;Gráfico de individuos&quot;)+ stat_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; En definitiva, las posibilidades son casi infinitas y evidentemente escapa a los propósitos de este manual. Si se desea saber más sobre este paquete, podemos consultar su manual oficial: ggplot2: Elegant Graphics for Data Analysis. 1.10 Las funciones en R Generalmente nuestros análisis consisten en una secuencia de comandos de R (posiblemente insertados dentro de un fichero .Rmd) que se ejecutan secuencialmente. En ocasiones, sin embargo, es conveniente crear funciones, por ejemplo, cuando hay operaciones comunes que se realizan reiteradamente (incluso en análisis distintos). Estas funciones podríamos equipararlas a las tan conocidas macros.Estas funciones en R requieren una bibliografía específica y son un capítulo aparte dentro de este lenguaje de programación, por lo que solo vamos a dar unas pinceladas para que se entiendan las funciones que se han plasmado a lo largo de este manual. Las funciones en R son tratadas como cualquier otro objeto y para crearlas utilizamos el comando function(), el cual creará un objeto de tipo function, para, a continuación llamarla, escribiendo el nombre de la misma. La estructura básica es la siguiente f &lt;- function(argumentos) { ## comandos de la función (body) } Veamos un ejemplo: f &lt;- function() { cat(&quot;Hola Mundo&quot;) } f() ## Hola Mundo En este punto ya habremos llegado a la conclusión de donde está el final del camino de las funciones: sí, en los paquetes. Un paquete no es más que un conjunto de funciones, ordenadas y estandarizadas, de tal forma que puedan ser replicables en todos nuestros análisis, y, si así lo decidimos que otros puedan usarlas si las compartimos libremente. 1.10.1 Estructura de una función Las funciones en R se componen de tres partes fundamentales: el cuerpo (body), los argumentos (formals) y el ambiente (environment) f &lt;- function(x, y) { x + y } body(f) { x + y } formals(f) $x $y environment(f) &lt;environment: R_GlobalEnv&gt; El cuerpo es la sucesión de códigos y comandos que la función ejecutará para realizar los propósitos que le pedimos, mientras que los argumentos son una serie de valores que se le pasan a la función de forma que ciertas variables dentro de la función tomen diversos valores. Ambios son obligatorios para confeccionar nuestra función. Es importante indicar que el orden con el que se incluyan en la declaración de la variables es como se ubicarán en la función: area_rectangulo &lt;- function(lado1, lado2) { area &lt;- lado1 * lado2 print(paste(&quot;el área es &quot;, area)) } area_rectangulo(lado1 = 2, lado2 = 3) ## [1] &quot;el área es 6&quot; &quot;el área es 6&quot; ## [1] &quot;el área es 6&quot; 1.11 Publicación de resultados Como hablamos, el sistema R no destaca especialmente por tratarse de un sistema visual y amigable para los usuarios noveles que se enfrentan a él. Ello se hace extensible a la publicación de los resultados, quizás el aspecto menos desarrollado del software. Esta carencia ha sido suplida, como no, por librerías que hacen más sencillo el formateo de los resultados, su publicación y divulgación. En concreto, y sobre cualquier otras, destacan las siguientes: knitr (Xie 2021), que permite la confección de informes dinámicos y reproducibles (esto es que se pueden modificar cada vez que rehagamos algo en nuestro código), que permite integrar el lenguaje R en otros documentos construidos en LaTeX, HTML, Markdown, Muy brevemente podríamos decir que knitr se basa en dos conceptos: el fragmento literal de texto, escrito comúnmente en Markdown y el fragmento de código R o chunk (la unión de ambos es lo que se conoce como RMarkdown). Una vez compilamos nuestro informe (proceso para lo cual se apoya en el software pandoc) tendremos, por un lado el texto formateado, según las características que hayamos definido en el encabezado del documento y los resultados de los códigos, embebidos en el mismo. Entre las salidas básicas tendremos el formato HTML, docx y pdf, si bien podremos generar muchos más gracias a las excepcionales posibilidades que da Pandoc. shiny (Chang et al. 2021), que se basa en el uso de Programación Reactiva (Reactive Programming), esto es, existen unos valores que pueden variar en el tiempo y que son registrados por unas expresiones que posteriormente las reproducen. Junto a este motor de razonamiento, incorpora, adaptadas, los clásicos elementos HTML (botones, cuadros de texto, barras de desplazamiento,) junto con las potentes gráficas de R y su estructura de datos. Básicamente se compone de dos ficheros que se crean en nuestro directorio local: ui.R, que gestiona la salida visual y el aspecto de la aplicación y server.R con las instrucciones para que la misma función, siendo en esta última donde se embebe el motor de cálculo en realizado en R. Estos archivos, que ya en las últimas versiones se encuentran embebidos en uno solo app.R, son publicados en nuestro servidor web o en las distintas plataformas que lo interpreten, como GitHub. Las posibilidades de shiny se amplían al admitir incorporaciones al código nativo de todos los elementos que actualmente conforman los ecosistemas HTML, CSS y Javascript, lo que implica que los desarrollos que se puedan hacer con esta herramienta son casi ilimitados. Esto la convierte en una herramienta muy poderosa para la construcción de cuadros de mando y para mostrar los resultados de nuestros desarrollos usando las capacidades analíticas de R, de forma que el receptor de nuestra información pueda interactuar fácilmente con ella. flexdashboards (Iannone, Allaire, and Borges 2020), que permite, de una forma extremadamente simple, crear cuadros de mando dinámicos e interactivos a partir de código Markdown. Muy parecido en concepto a shiny esta librería es adecuada si solo deseamos publicar nuestros resultados en formato de cuadro de mando y se busca (por ahora) una menor interacción del usuario. 1.11.1 La escritura con Markdown Markdown es un lenguaje de marcado ligero ideado en 2004 por John Gruber, con la visión de crear un lenguaje sencillo, fácil de leer y de escribir, aprovechando las ventajas del lenguaje HTML pero eliminando sus inconvenientes, como por ejemplo las etiquetas, que tanto engorran el código. Pongamonos en situación: si estamos escribiendo en HTML, para añadir una palabra en negrita deberíamos escribir &lt;strong&gt;importante&lt;/strong&gt; mientras que si estamos escribiendo en Markdownnos bastaría poner **importante** para que cuando lo compilemos, se muestre de esta forma. Huelga decir, que en un procesador de textos al uso deberemos dejar de escribir, coger el ratón, pulsar en el botón de la negrita, volver a escribir, de nuevo coger el ratón, ir a la barra de herramientas, desactivar el formato de negrita y continuar escribiendo (). A continuación podemos ver algunos ejemplos de la notación de este lenguaje: Referencias "],["r-y-la-información-lidar-forestal.html", "2 R y la información LiDAR forestal 2.1 La teledetección y la tecnología LiDAR 2.2 Preparación del entorno de trabajo 2.3 Preparación de los datos 2.4 Análisis de los datos 2.5 Modelo digital del terreno 2.6 Modelo digital de las elevaciones 2.7 Métodos de masa 2.8 Segmentación de árboles", " 2 R y la información LiDAR forestal 2.1 La teledetección y la tecnología LiDAR Entendemos por teledetección a toda aquella técnica que permite obtener información a distancia de objetos sin que exista un contacto material (por ejemplo una fotografía). Generalmente se aplica a objetos situados sobre la superficie terrestre y coloquialmente es sinónimo de información a partir de imágenes de satélite Teledetección pasiva: detectan radiación natural emitida o reflejada por el objeto o área circundante que está siendo observada. La luz solar reflejada es uno de los tipos de radiación más comunes medidos por esta clase de teledetección. Se necesita una fuente de energía, un objeto y un sensor teledetector. El primero ilumina el objetivo emitiendo una onda electromagnética (flujo de fotones) y el último mide la energía solar (es decir la radiación electromagnética) reflejada por el objetivo. Cuando la fuente de energía es el Sol, y el captador solo mide la radiación reflejada, se conoce como teledetección pasiva. ![]([https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/pasiva.png)](https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/markdown.png)) Teledetección activa: emiten energía que les permite escanear objetos y áreas. El sistema mide la radiación reflejada del objetivo. ](https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/pasiva.png))](https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/markdown.png)) Radar. Teledetector activo que mide el tiempo que tarda una emisión en ir y volver de un punto, estableciendo así la localización, altura, velocidad y dirección de un objeto determinado. LiDAR (Light Detection and Ranging o Laser Imaging Detection and Ranging). Dispositivo que permite determinar la distancia desde un emisor láser a un objeto o superficie utilizando un haz láser pulsado. 2.2 Preparación del entorno de trabajo A lo largo de este manual haremos uso de varias librerías de R relativas al tratamiento de datos espacial, con especial atención a la librería lidR que sera la herramienta de cabecera. Dado que muchas puede ser que ya estén instaladas en tu computadora, usaremos un pequeño código que permite distinguir si ya lo está y si no es así, procede a instalarla de forma transparente para ti. packages &lt;- c(&quot;lidR&quot;, &quot;sf&quot;, &quot;rgdal&quot;, &quot;dplyr&quot;, &quot;skimr&quot;, &quot;lmtest&quot;) # Instalamos los paquetes que no estén instalados installed_packages &lt;- packages %in% rownames(installed.packages()) if (any(installed_packages == FALSE)) { install.packages(packages[!installed_packages]) } # Cargamos los paquetes invisible(lapply(packages, library, character.only = TRUE)) A continuación crearemos una estructura de carpetas, predefinida, de forma que el trabajo sea lo más sistemático posible. Con ellas creadas, fijamos el directorio de trabajo y procedemos a descargar y descomprimir los datos LiDAR, vectoriales, raster y tabulares de los que disponemos del trabajo de campo y del vuelo LiDAR: dir.create(&quot;c:/curso_lidar/&quot;, showWarnings = FALSE) dir&lt;-&quot;c:/curso_lidar/&quot; setwd(dir) dir.create(&quot;./temp/&quot;, showWarnings = FALSE) dir.create(&quot;./data/&quot;, showWarnings = FALSE) setwd(&quot;./data/&quot;) download.file(&quot;https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/lidar_data.zip&quot;, destfile = paste(getwd(), &quot;/lidar_data.zip&quot;, sep=&quot;&quot;), method = &quot;curl&quot;, overwrite=TRUE) unzip(paste(getwd(), &quot;/lidar_data.zip&quot;,sep=&quot;&quot;)) setwd(dir) 2.3 Preparación de los datos Para comenzar, si analizamos los tiles LiDAR, estos no vienen proyectados, por lo que procederemos a ello. Para hacerlo de forma rápida y automática usaremos una estructura de control for de forma que si previamente hemos listado todos los ficheros LiDAR a reproyectar, abriremos cada uno de ellos con readLAS()se les asigne la proyección que realmente tienen y se guarden con ella en la carpeta temp con writeLAS. tiles &lt;- list.files(paste(dir, &quot;data/tiles/&quot;, sep=&quot;&quot;), full.names=TRUE) setwd(&quot;./temp/&quot;) getwd() for (file in tiles) { las&lt;-readLAS(file) epsg(las) &lt;- 25830 writeLAS(las, basename(file)) rm(las) } setwd(dir) 2.4 Análisis de los datos La librería lidR,para gestionar grandes cantidades de datos, usa un objeto LAScatalog de modo que el procesamiento de cada tesela LiDAR se hace de forma ordenada. El objeto LAScatalog en realidad contiene las referencias geográficas y cabeceras de los ficheros LiDAR que permite ubicar espacialmente y posteriormente aplicar la mayoría de las funciones de lidR como si se tratara de una sola nube de puntos cargada en la memoria de la computadora. ctg &lt;- readLAScatalog(&quot;./temp/&quot;) ctg plot(ctg) Una vez creado el objeto LAScatalog procedemos a su validación y estudio con el comando las_check: las_check(ctg) La clasificación de los puntos terrestres es un paso importante en el procesamiento de datos de nubes de puntos. La distinción entre puntos de suelo y no permite crear un modelo digital del terreno que posteriormente nos permita normalizar los puntos LiDAR, esto es, pasar de altitudes respecto a un determinado nivel de referencia a alturas o cotas respecto a un cero, que, evidentemente, si hablamos de árboles es lo que más nos puede interesar. Los datos del ejemplo ya vienen clasificados, pero los volveremos a clasificar para demostrar como lo efectúa la librería lidR. En concreto, esta librería usa dos formas de clasificación: pmf() (K. Zhang et al. 2003) y csf() (W. Zhang et al. 2016), cada uno con parámetros característicos que podremos modificar y es uno de los pasos más críticos y complejos del proceso. Una vez que los puntos son clasificados, se asigna el valor de 2 al campo de clasificación del retorno, siguiendo las prescripciones que estandariza la Sociedad Americana de Fotogrametría y Teledetección (ASPRS) para los formatos LAS 1.1, 1.2, 1.3 y 1.4.1 opt_filter(ctg) &lt;- &quot;-drop-scan-angle &#39;&gt;=91&#39;&quot; opt_output_files(ctg) &lt;- paste0(dir,&quot;/temp/&quot;, &quot;{*}_classified&quot;) classified_ctg &lt;- classify_ground(ctg, csf()) 2.5 Modelo digital del terreno Con la nube de puntos clasificada, al menos en puntos de suelo y no, calcularemos el modelo digital del terreno que posteriormente nos permita normalizar la nube de puntos. La librería lidRpermite tres métodos de interpolación de los puntos del suelo con el comando grid_terrain: tin (red de triángulos irregulares), knnidw (distancia inversa ponderada) y kriging, de menor a mayor complejidad y uso de recursos en este orden.2 En nuestro caso, usamos kriging (lidR usa la librería gstat por si deseamos saber más sobre este método de interpolación probabilístico) porque los otros métodos no se pueden usar debido a los pocos puntos de suelo que tenemos. Si usamos algorithm =tin() por ejemplo, observaremos a un error del tipo convexhull debido a que los puntos de suelo con los que contamos en muchos casos están muy separados físicamente. opt_output_files(classified_ctg) &lt;- paste0(dir,&quot;temp/&quot;, &quot;{*}_dtm&quot;) dtm &lt;- grid_terrain(classified_ctg, 1, algorithm = kriging(k = 40), overwrite=TRUE) dtm plot(dtm) 2.6 Modelo digital de las elevaciones # Normalizamos los datos y creamos el modelo digital de elevaciones normalizado opt_output_files(classified_ctg) &lt;- paste0(dir,&quot;/temp/&quot;, &quot;{*}_norm&quot;) ctg_norm &lt;- normalize_height(classified_ctg, dtm) ctg_norm las_check(ctg_norm) opt_filter(ctg_norm) &lt;- &quot;-drop_z_below 0&quot; # Ignoramos los puntos con cota por debajo de cero opt_output_files(ctg_norm) &lt;- paste0(dir,&quot;/temp/&quot;, &quot;{*}_mds&quot;) mdv &lt;- grid_canopy(ctg_norm, 1, pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2)) plot(mdv) 2.7 Métodos de masa La aplicación de la metodología de los métodos de masa consiste en relacionar unos lugares que conocemos a ciencia cierta, debido a que hemos procedido a medirlos in situ, relacionarlos con la nube de puntos en esos lugares y, sí existe una determinada relación (lineal o no), extrapolarla al resto de la nube de puntos de forma que podamos predecir el comportamiento de la masa forestal sin haberla evaluado desde el suelo. Es decir, se trata de una evaluación supervisada. Por comodidad y en adelante, estos lugares que vamos a recortar de la nube de puntos los denominaremos parcelas (de muestreo) y huelga hablar de la importancia de conocer con precisión y exactitud la ubicación de los mismos para que los puntos LiDAR que estudiemos sean los que exactamente han caído sobre los árboles medidos. opt_filter(ctg_norm) &lt;- &quot;-drop_z_below 0&quot; plots &lt;- st_read(&quot;./data/parc.shp&quot;) plots plot(plots, add = TRUE, col=&quot;red&quot;) Para recortar estas parcelas de la nube de puntos podemos usar el comando plot_metrics (además guardaremos estos recortes para su uso futuro): tiles_norm &lt;- list.files(paste(dir, &quot;temp/&quot;, sep=&quot;&quot;), full.names = TRUE, pattern = &quot;(.*)norm.las$&quot;) ctg_norm &lt;- readLAScatalog(tiles_norm) stats &lt;- plot_metrics(ctg_norm, .stdmetrics_z, plots, radius = 9) head(stats) Cargamos los datos medidos en las parcelas y se lo añadimos a los datos lidar. A continuación realizamos un análisis de correlación entre las variables de la nube de puntos y la futura variable dependiente (en este caso vcc). Se define por correlación a la dirección y magnitud de la asociación entre dos variables cuantitativas, es decir que grado de relación hay entre ellas y si el aumento o disminución de una provoca el aumento o disminución de la otra. El grado de correlación se mide mediante dos coeficientes: el coeficiente de correlación de Pearson y el coeficiente de correlación de Spearman, análogos en significado pero disimiles en su aplicación. El coeficiente de correlación de Pearson es un coeficiente adimensional que va fluctúa entre -1 y +1, donde el primer valor indica la existencia de una asociación perfecta en sentido decreciente y el segundo en sentido creciente. Evidentemente, el valor cero, indica la inexistencia de relación. El coeficiente de correlación de Pearson (a veces conocido como coeficiente de correlación a secas) es aplicable para variables cuantitativas que se relacionan de forma lineal. Por otro lado, el coeficiente de correlación de Spearman, es un evaluador de correlación, homólogo en su valor (-1 a +1) al de Pearson, pero con un carácter no paramétrico, lo que implica que la relación entre las variables puede no ser lineal, que no es tan exigente en la existencia de normalidad en la población y aplicable incluso a variables ordinales. Centrándonos en el coeficiente de correlación de Pearson, dado que tratamos variables cuantitativas, empezamos cargando los datos de volumenes de las parcelas de muestreo, pasando posteriormente a unirlas a la tabla con los estadísticos descriptivos de la nube de puntos de esas parcelas obtenido anteriormente. Para finalizar mostramos las seis variables más relacionadas con la futura variable dependiente. vcc &lt;- readxl::read_excel(&quot;data/muestreo.xlsx&quot;) head(vcc) vcc &lt;- left_join(vcc, stats, by = c(&quot;parc&quot; = &quot;id&quot;)) df &lt;- vcc %&gt;% select(-c(geometry, parc, X, Y))%&gt;% cor()%&gt;% data.frame()%&gt;% add_rownames(var = &quot;lidar_stat&quot;)%&gt;% filter(lidar_stat!=&#39;vcc&#39;)%&gt;% arrange(desc(vcc)) head(df, 3) tail(df, 3) Analizamos la variable dependiente skim(vcc$vcc) shapiro.test(vcc$vcc) plotn &lt;- function(x,main=&quot;Histograma de frecuencias \\ny distribución normal&quot;, xlab=&quot;X&quot;,ylab=&quot;Densidad&quot;) { min &lt;- min(x) max &lt;- max(x) media &lt;- mean(x) dt &lt;- sd(x) hist(x,freq=F,main=main,xlab=xlab,ylab=ylab) curve(dnorm(x,media,dt), min, max,add = T,col=&quot;blue&quot;) } plotn(sqrt(vcc$vcc)) Construimos el modelo lm&lt;-lm(vcc~zq90, data=vcc) summary(lm) plot(vcc$vcc, predict(lm)) abline(0,1) par(mfrow=c(2,2)) plot(lm) par(mfrow=c(1,1)) shapiro.test(residuals(lm)) bptest(lm) Calculamos los estadísticos de los recintos. El radio de nuestras parcelas es de 9 metros, por lo que la superficie es de 254.469004941 m2 y esto implica una resolución de 15.95208 pixel &lt;- sqrt(pi*9^2) metrics &lt;- grid_metrics(ctg_norm, .stdmetrics_z, res = pixel) plot(metrics$zq90) vcc_pred &lt;- -899.025+73.630*metrics$zq90 plot(vcc_pred) writeRaster(vcc_pred, paste(dir, &quot;/temp/&quot;, &quot;vcc_pred.tif&quot;, sep=&quot;&quot;), datatype=&#39;FLT4S&#39;,overwrite = TRUE, bylayer = FALSE) 2.8 Segmentación de árboles opt_output_files(ctg_norm) &lt;- &quot;&quot; ttops &lt;- find_trees(ctg_norm, lmf(4), uniqueness = &quot;bitmerge&quot;) opt_output_files(ctg_norm) &lt;- paste0(dir,&quot;/temp/&quot;, &quot;{*}_segmented&quot;) segm &lt;- dalponte2016(mdv, ttops) ctg_segmented &lt;- segment_trees(ctg_norm, segm) opt_output_files(ctg_segmented) &lt;- &quot;&quot; lasplot &lt;- clip_circle(ctg_segmented, 599358.8984,4734939.2286, 50) #plot(lasplot, color = &quot;treeID&quot;, bg = &quot;white&quot;, size = 4) Referencias "],["references.html", "3 Referencias", " 3 Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
