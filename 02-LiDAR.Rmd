# R y la información LiDAR forestal

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
##knitr::opts_knit$set(root.dir = 'c:/curso_lidar/')
```

## La teledetección y la tecnología LiDAR

Entendemos por teledetección a toda aquella técnica que permite obtener información a distancia de objetos sin que exista un contacto material (por ejemplo una fotografía). Generalmente se aplica a objetos situados sobre la superficie terrestre y coloquialmente es sinónimo de información a partir de imágenes de satélite

-   Teledetección pasiva: detectan radiación natural emitida o reflejada por el objeto o área circundante que está siendo observada. La luz solar reflejada es uno de los tipos de radiación más comunes medidos por esta clase de teledetección.

Se necesita una fuente de energía, un objeto y un sensor teledetector. El primero "ilumina" el objetivo emitiendo una onda electromagnética (flujo de fotones) y el último mide la energía solar (es decir la radiación electromagnética) reflejada por el objetivo. Cuando la fuente de energía es el Sol, y el captador solo mide la radiación reflejada, se conoce como teledetección pasiva.

![](https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/pasiva.png){width="294"}

-   Teledetección activa: emiten energía que les permite "escanear" objetos y áreas. El sistema mide la radiación reflejada del objetivo.

    -   Radar. Teledetector activo que mide el tiempo que tarda una emisión en ir y volver de un punto, estableciendo así la localización, altura, velocidad y dirección de un objeto determinado.

    -   LiDAR (Light Detection and Ranging o Laser Imaging Detection and Ranging). Dispositivo que permite determinar la distancia desde un emisor láser a un objeto o superficie utilizando un haz láser pulsado.

![](https://raw.githubusercontent.com/juliomsevilla/rlidar/main/images/activa.png){width="294"}

## Preparación del entorno de trabajo

A lo largo de este manual haremos uso de varias librerías de `R` relativas al tratamiento de datos espacial, con especial atención a la librería `lidR` que sera la herramienta de cabecera. Dado que muchas puede ser que ya estén instaladas en tu computadora, usaremos un pequeño código que permite distinguir si ya lo está y si no es así, procede a instalarla de forma transparente para ti.

```{r libraries, message=FALSE, eval = FALSE}

packages <- c("lidR", "sf", "rgdal", "dplyr", "skimr", "lmtest")

# Instalamos los paquetes que no estén instalados
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Cargamos los paquetes
invisible(lapply(packages, library, character.only = TRUE))

```

A continuación crearemos una estructura de carpetas, predefinida, de forma que el trabajo sea lo más sistemático posible. Con ellas creadas, fijamos el directorio de trabajo y procedemos a descargar y descomprimir los datos LiDAR, vectoriales, raster y tabulares de los que disponemos del trabajo de campo y del vuelo LiDAR:

```{r directorios, eval = FALSE}
dir.create("c:/curso_lidar/", showWarnings = FALSE)


dir<-"c:/curso_lidar/"
setwd(dir)

dir.create("./temp/", showWarnings = FALSE)
dir.create("./data/", showWarnings = FALSE)

setwd("./data/")
download.file("https://raw.githubusercontent.com/juliomsevilla/rlidar/main/data/lidar_data.zip",
              destfile = paste(getwd(), "/lidar_data.zip", sep=""), 
              method = "curl", overwrite=TRUE)

unzip(paste(getwd(), "/lidar_data.zip",sep=""))
setwd(dir)

```

## Preparación de los datos

Para comenzar, si analizamos los *tiles* LiDAR, estos no vienen proyectados, por lo que procederemos a ello. Para hacerlo de forma rápida y automática usaremos una estructura de control `for` de forma que si previamente hemos listado todos los ficheros LiDAR a reproyectar, abriremos cada uno de ellos con `readLAS()`se les asigne la proyección que realmente tienen y se guarden con ella en la carpeta `temp` con `writeLAS`.

```{r proyeccion, eval = FALSE}
tiles <- list.files(paste(dir, "data/tiles/", sep=""), full.names=TRUE)
setwd("./temp/")
getwd()


for (file in tiles) {
  
  las<-readLAS(file)
  epsg(las) <- 25830
  writeLAS(las, basename(file))
  rm(las)
}

setwd(dir)
```

## Análisis de los datos

La librería `lidR`,para gestionar grandes cantidades de datos, usa un objeto `LAScatalog` de modo que el procesamiento de cada tesela LiDAR se hace de forma ordenada. El objeto `LAScatalog` en realidad contiene las referencias geográficas y cabeceras de los ficheros LiDAR que permite ubicar espacialmente y posteriormente aplicar la mayoría de las funciones de `lidR` como si se tratara de una sola nube de puntos cargada en la memoria de la computadora.

```{r catalogo, eval = FALSE}
ctg <- readLAScatalog("./temp/")
ctg
plot(ctg)
```

Una vez creado el objeto `LAScatalog` procedemos a su validación y estudio con el comando `las_check`:

```{r lascheck, eval = FALSE}
las_check(ctg)
```

La clasificación de los puntos terrestres es un paso importante en el procesamiento de datos de nubes de puntos. La distinción entre puntos de suelo y no permite crear un modelo digital del terreno que posteriormente nos permita normalizar los puntos LiDAR, esto es, pasar de altitudes respecto a un determinado nivel de referencia a alturas o cotas respecto a un cero, que, evidentemente, si hablamos de árboles es lo que más nos puede interesar.

Los datos del ejemplo ya vienen clasificados, pero los volveremos a clasificar para demostrar como lo efectúa la librería `lidR`. En concreto, esta librería usa dos formas de clasificación: `pmf()` [@R-Zhang2003] y `csf()` [@R-Zhang2016], cada uno con parámetros característicos que podremos modificar y es uno de los pasos más críticos y complejos del proceso.

Una vez que los puntos son clasificados, se asigna el valor de 2 al campo de clasificación del retorno, siguiendo las prescripciones que estandariza la Sociedad Americana de Fotogrametría y Teledetección ([ASPRS](https://www.asprs.org/)) para los formatos LAS 1.1, 1.2, 1.3 y 1.4.[^lidar-1]

[^lidar-1]: El comando `classify_ground` no se limita solo a estos dos algoritmos, si no que la librería `lidR` permite añadir todo aquel algoritmo del que dispongamos fácilmente.

```{r clasificacion,  eval = FALSE}
opt_filter(ctg) <- "-drop-scan-angle '>=91'"
opt_output_files(ctg) <- paste0(dir,"/temp/", "{*}_classified")
classified_ctg <- classify_ground(ctg, csf())
```

```{r clasificacion_previa, echo=FALSE, eval = FALSE}
classified_ctg <- ctg
```

## Modelo digital del terreno

Con la nube de puntos clasificada, al menos en puntos de suelo y no, calcularemos el modelo digital del terreno que posteriormente nos permita normalizar la nube de puntos.

La librería `lidR`permite tres métodos de interpolación de los puntos del suelo con el comando `grid_terrain`: `tin` (red de triángulos irregulares), `knnidw` (distancia inversa ponderada) y `kriging`, de menor a mayor complejidad y uso de recursos en este orden. [^lidar-2]

[^lidar-2]: El comando `grid_terrain` no se limita solo a estos tres algoritmos, si no que la librería `lidR` permite añadir todo aquel algoritmo del que dispongamos fácilmente.

En nuestro caso, usamos `kriging` (`lidR` usa la librería `gstat` por si deseamos saber más sobre este método de interpolación probabilístico) porque los otros métodos no se pueden usar debido a los pocos puntos de suelo que tenemos. Si usamos `algorithm =tin()` por ejemplo, observaremos a un error del tipo `convexhull` debido a que los puntos de suelo con los que contamos en muchos casos están muy separados físicamente.

```{r mdt, eval = FALSE}
opt_output_files(classified_ctg) <- paste0(dir,"temp/", "{*}_dtm")

dtm <- grid_terrain(classified_ctg, 1,  algorithm = kriging(k = 40), overwrite=TRUE)
dtm
plot(dtm)

```

## Modelo digital de las elevaciones

```{r mde, eval = FALSE}
# Normalizamos los datos y creamos el modelo digital de elevaciones normalizado
opt_output_files(classified_ctg) <- paste0(dir,"/temp/", "{*}_norm")
ctg_norm <- normalize_height(classified_ctg, dtm)
ctg_norm
las_check(ctg_norm)


opt_filter(ctg_norm) <- "-drop_z_below 0" # Ignoramos los puntos con cota por debajo de cero
opt_output_files(ctg_norm) <- paste0(dir,"/temp/", "{*}_mds")
mdv <- grid_canopy(ctg_norm, 1, pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2))
plot(mdv)

```

## Métodos de masa

La aplicación de la metodología de los métodos de masa consiste en relacionar unos lugares que conocemos a ciencia cierta, debido a que hemos procedido a medirlos in situ, relacionarlos con la nube de puntos en esos lugares y, sí existe una determinada relación (lineal o no), extrapolarla al resto de la nube de puntos de forma que podamos predecir el comportamiento de la masa forestal sin haberla evaluado desde el suelo. Es decir, se trata de una evaluación supervisada.

Por comodidad y en adelante, estos lugares que vamos a recortar de la nube de puntos los denominaremos parcelas (de muestreo) y huelga hablar de la importancia de conocer con precisión y exactitud la ubicación de los mismos para que los puntos LiDAR que estudiemos sean los que exactamente han caído sobre los árboles medidos.

```{r plots, eval = FALSE}
opt_filter(ctg_norm) <- "-drop_z_below 0"

plots <- st_read("./data/parc.shp")
plots
plot(plots, add = TRUE, col="red")
```

Para recortar estas parcelas de la nube de puntos podemos usar el comando `plot_metrics` (además guardaremos estos recortes para su uso futuro):

```{r clipdata, eval = FALSE}
tiles_norm <- list.files(paste(dir, "temp/", sep=""), full.names = TRUE, pattern = "(.*)norm.las$")
ctg_norm <- readLAScatalog(tiles_norm)
stats <- plot_metrics(ctg_norm, .stdmetrics_z, plots, radius = 9)
head(stats)
```

Cargamos los datos medidos en las parcelas y se lo añadimos a los datos lidar. A continuación realizamos un análisis de correlación entre las variables de la nube de puntos y la futura variable dependiente (en este caso `vcc`).

Se define por correlación a la dirección y magnitud de la asociación entre dos variables cuantitativas, es decir que grado de relación hay entre ellas y si el aumento o disminución de una provoca el aumento o disminución de la otra.

El grado de correlación se mide mediante dos coeficientes: el coeficiente de correlación de Pearson y el coeficiente de correlación de Spearman, análogos en significado pero disimiles en su aplicación.

El coeficiente de correlación de Pearson es un coeficiente adimensional que va fluctúa entre -1 y +1, donde el primer valor indica la existencia de una asociación perfecta en sentido decreciente y el segundo en sentido creciente. Evidentemente, el valor cero, indica la inexistencia de relación.

El coeficiente de correlación de Pearson (a veces conocido como coeficiente de correlación a secas) es aplicable para variables cuantitativas que se relacionan de forma lineal. Por otro lado, el coeficiente de correlación de Spearman, es un evaluador de correlación, homólogo en su valor (-1 a +1) al de Pearson, pero con un carácter no paramétrico, lo que implica que la relación entre las variables puede no ser lineal, que no es tan exigente en la existencia de normalidad en la población y aplicable incluso a variables ordinales.

Centrándonos en el coeficiente de correlación de Pearson, dado que tratamos variables cuantitativas, empezamos cargando los datos de volumenes de las parcelas de muestreo, pasando posteriormente a unirlas a la tabla con los estadísticos descriptivos de la nube de puntos de esas parcelas obtenido anteriormente. Para finalizar mostramos las seis variables más relacionadas con la futura variable dependiente.

```{r correlaciones, eval = FALSE}
vcc <- readxl::read_excel("data/muestreo.xlsx")
head(vcc)

vcc <- left_join(vcc, stats, by = c("parc" = "id"))

df <- vcc %>% 
      select(-c(geometry, parc, X, Y))%>%
      cor()%>%
      data.frame()%>%
      add_rownames(var = "lidar_stat")%>%
      filter(lidar_stat!='vcc')%>%
      arrange(desc(vcc))


head(df, 3)
tail(df, 3)
```

Analizamos la variable dependiente

```{r normalidad, eval = FALSE}
skim(vcc$vcc)
shapiro.test(vcc$vcc)

plotn <- function(x,main="Histograma de frecuencias \ny distribución normal",
                  xlab="X",ylab="Densidad") {
  min <- min(x)
  max <- max(x)
  media <- mean(x)
  dt <- sd(x)
  hist(x,freq=F,main=main,xlab=xlab,ylab=ylab)
  curve(dnorm(x,media,dt), min, max,add = T,col="blue")
}

plotn(sqrt(vcc$vcc))
```

Construimos el modelo

```{r linear_model, eval = FALSE}
lm<-lm(vcc~zq90, data=vcc)
summary(lm)

plot(vcc$vcc, predict(lm))
abline(0,1)

par(mfrow=c(2,2)) 
plot(lm)
par(mfrow=c(1,1))

shapiro.test(residuals(lm))
bptest(lm)
```

Calculamos los estadísticos de los recintos. El radio de nuestras parcelas es de 9 metros, por lo que la superficie es de 254.469004941 m2 y esto implica una resolución de 15.95208

```{r gridmetrics, eval = FALSE}
pixel <- sqrt(pi*9^2)

metrics <- grid_metrics(ctg_norm, .stdmetrics_z, res = pixel)

plot(metrics$zq90)
vcc_pred <- -899.025+73.630*metrics$zq90
plot(vcc_pred)

writeRaster(vcc_pred, paste(dir, "/temp/", "vcc_pred.tif", sep=""), datatype='FLT4S',overwrite = TRUE, bylayer = FALSE)
```

## Segmentación de árboles

```{r segmentation, eval = FALSE}
opt_output_files(ctg_norm) <- ""
ttops <- find_trees(ctg_norm, lmf(4), uniqueness = "bitmerge")

opt_output_files(ctg_norm) <- paste0(dir,"/temp/", "{*}_segmented")
segm <- dalponte2016(mdv, ttops)
ctg_segmented <- segment_trees(ctg_norm, segm)

opt_output_files(ctg_segmented) <- ""
lasplot <- clip_circle(ctg_segmented, 599358.8984,4734939.2286,  50)
#plot(lasplot, color = "treeID", bg = "white", size = 4)
```
